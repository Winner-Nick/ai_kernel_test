"""
åˆ†æåŒ…å«å®Œæ•´è¯„ä¼°çš„æµ‹è¯•ç»“æœ
"""

import json
import os
from pathlib import Path
from datetime import datetime


def load_latest_results(results_dir='./results', pattern='test_results_with_eval_*.json'):
    """åŠ è½½æœ€æ–°çš„æµ‹è¯•ç»“æœ"""
    results_path = Path(results_dir)
    if not results_path.exists():
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
        return None

    # æ‰¾åˆ°æœ€æ–°çš„ç»“æœæ–‡ä»¶
    result_files = list(results_path.glob(pattern))
    if not result_files:
        print(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•ç»“æœæ–‡ä»¶: {pattern}")
        return None

    latest_file = max(result_files, key=lambda p: p.stat().st_mtime)
    print(f"ğŸ“‚ åŠ è½½ç»“æœæ–‡ä»¶: {latest_file.name}")

    with open(latest_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def analyze_performance(results_data):
    """è¯¦ç»†åˆ†ææ€§èƒ½æ•°æ®"""
    print("\n" + "=" * 80)
    print("ğŸ“Š KernelBench å®Œæ•´è¯„ä¼°ç»“æœåˆ†æ")
    print("=" * 80)

    problem_name = results_data.get('problem_name', 'Unknown')
    test_config = results_data.get('test_config', {})
    results = results_data.get('results', [])

    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {problem_name}")
    print(f"ğŸ¯ Level: {test_config.get('level')}, Problem ID: {test_config.get('problem_id')}")
    print(f"ğŸ”§ Backend: {test_config.get('backend')}")
    print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {results_data.get('timestamp')}")

    # æ•´ä½“ç»Ÿè®¡
    total = len(results)
    generated = sum(1 for r in results if r['success'])
    evaluated = sum(1 for r in results if r.get('eval_result') is not None)
    correct = sum(1 for r in results if r.get('eval_result') and r['eval_result'].get('is_correct'))
    fast_1 = sum(1 for r in results if r.get('eval_result') and r['eval_result'].get('fast_1'))
    fast_2 = sum(1 for r in results if r.get('eval_result') and r['eval_result'].get('fast_2'))

    print(f"\nğŸ“ˆ æ•´ä½“ç»Ÿè®¡:")
    print(f"  æµ‹è¯•æ¨¡å‹æ•°: {total}")
    print(f"  æˆåŠŸç”Ÿæˆä»£ç : {generated}/{total} ({generated/total*100:.1f}%)")
    print(f"  å®Œæˆè¯„ä¼°: {evaluated}/{total} ({evaluated/total*100:.1f}%)")
    print(f"  âœ… fast_0 (æ­£ç¡®æ€§): {correct}/{total} ({correct/total*100:.1f}%)")
    print(f"  ğŸš€ fast_1 (æ­£ç¡® ä¸” å¿«äºPyTorch): {fast_1}/{total} ({fast_1/total*100:.1f}%)")
    print(f"  ğŸ† fast_2 (æ­£ç¡® ä¸” å¿«2å€ä»¥ä¸Š): {fast_2}/{total} ({fast_2/total*100:.1f}%)")

    # è¯¦ç»†è¡¨æ ¼
    print("\n" + "-" * 100)
    print(f"{'æ¨¡å‹':<30} {'ç”Ÿæˆ':<6} {'æ­£ç¡®':<6} {'åŠ é€Ÿæ¯”':<10} {'PyTorch(ms)':<12} {'ç”Ÿæˆ(ms)':<12} {'fast_1':<6}")
    print("-" * 100)

    for result in results:
        name = result['model_name'][:28]
        gen = "âœ…" if result['success'] else "âŒ"

        if result.get('eval_result'):
            ev = result['eval_result']
            correct_mark = "âœ…" if ev.get('is_correct') else "âŒ"
            speedup = f"{ev.get('speedup', 0):.2f}x"
            ref_time = f"{ev.get('ref_time_ms', 0):.4f}"
            custom_time = f"{ev.get('custom_time_ms', 0):.4f}"
            fast1 = "âœ…" if ev.get('fast_1') else "âŒ"
        else:
            correct_mark = "N/A"
            speedup = "N/A"
            ref_time = "N/A"
            custom_time = "N/A"
            fast1 = "N/A"

        print(f"{name:<30} {gen:<6} {correct_mark:<6} {speedup:<10} {ref_time:<12} {custom_time:<12} {fast1:<6}")

    # æ€§èƒ½æ’å
    correct_results = [r for r in results
                      if r.get('eval_result') and r['eval_result'].get('is_correct')]

    if correct_results:
        print("\n" + "=" * 80)
        print("ğŸ… æ€§èƒ½æ’å (ä»…æ­£ç¡®çš„å®ç°)")
        print("=" * 80)

        # æŒ‰åŠ é€Ÿæ¯”æ’åº
        correct_results.sort(key=lambda x: x['eval_result']['speedup'], reverse=True)

        print(f"\n{'æ’å':<6} {'æ¨¡å‹':<35} {'åŠ é€Ÿæ¯”':<10} {'å®é™…æ—¶é—´(ms)':<15}")
        print("-" * 80)
        for i, result in enumerate(correct_results, 1):
            ev = result['eval_result']
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
            name = result['model_name'][:33]
            speedup = f"{ev['speedup']:.2f}x"
            time = f"{ev['custom_time_ms']:.4f}"
            print(f"{medal:<6} {name:<35} {speedup:<10} {time:<15}")

        # åŠ é€Ÿæ¯”å¯è§†åŒ–
        print("\nåŠ é€Ÿæ¯”å¯è§†åŒ– (ç›¸å¯¹äº PyTorch):")
        max_speedup = max(r['eval_result']['speedup'] for r in correct_results)
        for result in correct_results[:10]:  # åªæ˜¾ç¤ºå‰10
            ev = result['eval_result']
            name = result['model_name'][:25]
            speedup = ev['speedup']
            bar_length = int((speedup / max_speedup) * 50)
            bar = 'â–ˆ' * bar_length
            print(f"  {name:<25} {bar} {speedup:.2f}x")

    # å¤±è´¥åˆ†æ
    failed_results = [r for r in results if not r['success']]
    eval_failed = [r for r in results
                  if r['success'] and (not r.get('eval_result') or not r['eval_result'].get('is_correct'))]

    if failed_results:
        print("\n" + "=" * 80)
        print("âŒ ä»£ç ç”Ÿæˆå¤±è´¥åˆ†æ")
        print("=" * 80)
        for result in failed_results:
            print(f"\næ¨¡å‹: {result['model_name']}")
            print(f"é”™è¯¯: {result['error']}")

    if eval_failed:
        print("\n" + "=" * 80)
        print("âš ï¸  è¯„ä¼°å¤±è´¥æˆ–æ­£ç¡®æ€§æœªé€šè¿‡")
        print("=" * 80)
        for result in eval_failed:
            print(f"\næ¨¡å‹: {result['model_name']}")
            if result.get('error'):
                print(f"é”™è¯¯: {result['error']}")
            elif result.get('eval_result') and not result['eval_result'].get('is_correct'):
                print(f"åŸå› : æ­£ç¡®æ€§æµ‹è¯•æœªé€šè¿‡")
            else:
                print(f"åŸå› : æœªçŸ¥è¯„ä¼°é”™è¯¯")

    # ç”Ÿæˆæ—¶é—´åˆ†æ
    generated_results = [r for r in results if r['success'] and r.get('generation_time')]
    if generated_results:
        print("\n" + "=" * 80)
        print("â±ï¸  ä»£ç ç”Ÿæˆé€Ÿåº¦åˆ†æ")
        print("=" * 80)

        times = [r['generation_time'] for r in generated_results]
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)

        print(f"\nå¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f}s")
        print(f"æœ€å¿«: {min_time:.2f}s")
        print(f"æœ€æ…¢: {max_time:.2f}s")

        # æ’åº
        generated_results.sort(key=lambda x: x['generation_time'])
        print(f"\nç”Ÿæˆé€Ÿåº¦æ’å:")
        for i, result in enumerate(generated_results[:5], 1):  # å‰5å
            print(f"  {i}. {result['model_name']:<30} {result['generation_time']:.2f}s")

    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 80)


def generate_markdown_report(results_data, output_file='results/report_with_eval.md'):
    """ç”Ÿæˆ Markdown æ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# AI æ¨¡å‹ GPU æ ¸å‡½æ•°ç”Ÿæˆèƒ½åŠ›å®Œæ•´æµ‹è¯•æŠ¥å‘Š\n\n")
        f.write("## ä½¿ç”¨ KernelBench å®˜æ–¹è¯„ä¼°ï¼ˆæ­£ç¡®æ€§ + æ€§èƒ½ï¼‰\n\n")

        problem_name = results_data.get('problem_name', 'Unknown')
        test_config = results_data.get('test_config', {})
        results = results_data.get('results', [])

        f.write(f"### æµ‹è¯•ä¿¡æ¯\n\n")
        f.write(f"- **é—®é¢˜**: {problem_name}\n")
        f.write(f"- **Level**: {test_config.get('level')}\n")
        f.write(f"- **Problem ID**: {test_config.get('problem_id')}\n")
        f.write(f"- **Backend**: {test_config.get('backend')}\n")
        f.write(f"- **æµ‹è¯•æ—¶é—´**: {results_data.get('timestamp')}\n\n")

        # ç»Ÿè®¡
        total = len(results)
        correct = sum(1 for r in results if r.get('eval_result') and r['eval_result'].get('is_correct'))
        fast_1 = sum(1 for r in results if r.get('eval_result') and r['eval_result'].get('fast_1'))
        fast_2 = sum(1 for r in results if r.get('eval_result') and r['eval_result'].get('fast_2'))

        f.write(f"### æ•´ä½“ç»“æœ\n\n")
        f.write(f"- æµ‹è¯•æ¨¡å‹æ•°: {total}\n")
        f.write(f"- **fast_0** (æ­£ç¡®æ€§): {correct}/{total} ({correct/total*100:.1f}%)\n")
        f.write(f"- **fast_1** (å¿«äºPyTorch): {fast_1}/{total} ({fast_1/total*100:.1f}%)\n")
        f.write(f"- **fast_2** (å¿«2å€ä»¥ä¸Š): {fast_2}/{total} ({fast_2/total*100:.1f}%)\n\n")

        f.write(f"### è¯¦ç»†ç»“æœ\n\n")
        f.write("| æ¨¡å‹ | ç”Ÿæˆ | æ­£ç¡® | åŠ é€Ÿæ¯” | PyTorchæ—¶é—´(ms) | ç”Ÿæˆä»£ç æ—¶é—´(ms) | fast_1 |\n")
        f.write("|------|------|------|--------|----------------|-----------------|--------|\n")

        for result in results:
            name = result['model_name']
            gen = "âœ…" if result['success'] else "âŒ"

            if result.get('eval_result'):
                ev = result['eval_result']
                correct_mark = "âœ…" if ev.get('is_correct') else "âŒ"
                speedup = f"{ev.get('speedup', 0):.2f}x"
                ref_time = f"{ev.get('ref_time_ms', 0):.4f}"
                custom_time = f"{ev.get('custom_time_ms', 0):.4f}"
                fast1 = "âœ…" if ev.get('fast_1') else "âŒ"
            else:
                correct_mark = "N/A"
                speedup = "N/A"
                ref_time = "N/A"
                custom_time = "N/A"
                fast1 = "N/A"

            f.write(f"| {name} | {gen} | {correct_mark} | {speedup} | {ref_time} | {custom_time} | {fast1} |\n")

        # æ€§èƒ½æ’å
        correct_results = [r for r in results
                          if r.get('eval_result') and r['eval_result'].get('is_correct')]
        if correct_results:
            correct_results.sort(key=lambda x: x['eval_result']['speedup'], reverse=True)

            f.write(f"\n### æ€§èƒ½æ’å (ä»…æ­£ç¡®çš„å®ç°)\n\n")
            f.write("| æ’å | æ¨¡å‹ | åŠ é€Ÿæ¯” | å®é™…æ—¶é—´(ms) |\n")
            f.write("|------|------|--------|-------------|\n")

            for i, result in enumerate(correct_results, 1):
                ev = result['eval_result']
                name = result['model_name']
                speedup = f"{ev['speedup']:.2f}x"
                time = f"{ev['custom_time_ms']:.4f}"
                f.write(f"| {i} | {name} | {speedup} | {time} |\n")

        f.write(f"\n---\n\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")

    print(f"\nğŸ“ Markdown æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    results_data = load_latest_results()

    if not results_data:
        print("âŒ æ— æ³•åŠ è½½æµ‹è¯•ç»“æœ")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ 'python test_models_with_eval.py' è¿›è¡Œæµ‹è¯•")
        return

    # ç”Ÿæˆæ§åˆ¶å°åˆ†ææŠ¥å‘Š
    analyze_performance(results_data)

    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    generate_markdown_report(results_data)


if __name__ == "__main__":
    main()
