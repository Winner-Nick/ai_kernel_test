"""
ç”Ÿæˆ baseline æ—¶é—´ - åªæµ‹é‡ config é‡Œå®šä¹‰çš„é—®é¢˜
å®Œå…¨ç…§æ¬ KernelBench çš„ generate_baseline_time.py
"""
import sys
import os
import json
import torch

# é…ç½® HuggingFace é•œåƒ
os.environ['HF_ENDPOINT'] = os.getenv('HF_ENDPOINT', 'https://hf-mirror.com')

# æ·»åŠ  KernelBench åˆ°è·¯å¾„
kernelbench_path = os.path.join(os.path.dirname(__file__), '../KernelBench')
sys.path.insert(0, kernelbench_path)

from datasets import load_dataset
from src.eval import load_original_model_and_inputs, time_execution_with_cuda_event, get_timing_stats, set_seed
from src.utils import set_gpu_arch

from kernelbench_wrapper.config import TEST_CONFIG, OUTPUT_DIR


def measure_reference_time(ref_code, num_trials=100, device=None, backend='cuda', verbose=False):
    """
    æµ‹é‡å‚è€ƒå®ç°çš„æ€§èƒ½ - ç…§æ¬ KernelBench çš„ measure_program_time

    Args:
        ref_code: å‚è€ƒä»£ç 
        num_trials: æµ‹è¯•æ¬¡æ•°
        device: CUDA è®¾å¤‡
        backend: åç«¯ç±»å‹
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        dict: è¿è¡Œæ—¶ç»Ÿè®¡ä¿¡æ¯
    """
    if device is None:
        device = torch.cuda.current_device()

    context = {}
    Model, get_init_inputs, get_inputs = load_original_model_and_inputs(ref_code, context)

    if Model is None or get_init_inputs is None or get_inputs is None:
        raise ValueError("å‚è€ƒä»£ç æ— æ³•åŠ è½½å¿…éœ€çš„å‡½æ•°")

    try:
        with torch.no_grad():
            torch.cuda.synchronize(device=device)
            set_seed(42)
            inputs = get_inputs()
            set_seed(42)
            init_inputs = get_init_inputs()

            # å°†è¾“å…¥ç§»åˆ° GPU
            inputs = [
                x.cuda(device=device) if isinstance(x, torch.Tensor) else x
                for x in inputs
            ]
            init_inputs = [
                x.cuda(device=device) if isinstance(x, torch.Tensor) else x
                for x in init_inputs
            ]

            # åˆå§‹åŒ–æ¨¡å‹ï¼ˆPyTorch Eager æ¨¡å¼ï¼‰
            model = Model(*init_inputs)
            model = model.cuda(device=device)
            torch.cuda.synchronize(device=device)

            # ä½¿ç”¨ CUDA Event è®¡æ—¶
            elapsed_times = time_execution_with_cuda_event(
                model, *inputs, num_trials=num_trials, verbose=verbose, device=device
            )
            runtime_stats = get_timing_stats(elapsed_times, device=device)

            if verbose:
                print(f"å‚è€ƒå®ç°æ—¶é—´ç»Ÿè®¡: {runtime_stats}")

            return runtime_stats

    except Exception as e:
        print(f"âŒ æµ‹é‡å‚è€ƒå®ç°æ€§èƒ½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å‡½æ•° - ç”Ÿæˆ baseline æ—¶é—´"""
    print("=" * 80)
    print("â±ï¸  ç”Ÿæˆ Baseline æ—¶é—´ (å‚è€ƒå®ç°æ€§èƒ½)")
    print("   ç…§æ¬ KernelBench çš„ generate_baseline_time.py")
    print("=" * 80)

    # æ£€æŸ¥ GPU
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ° CUDA GPUï¼")
        return

    device = torch.cuda.current_device()
    print(f"âœ… GPU: {torch.cuda.get_device_name(device)}")

    # è®¾ç½® GPU æ¶æ„
    if TEST_CONFIG.get('gpu_arch'):
        set_gpu_arch(TEST_CONFIG['gpu_arch'])

    # è·å–é—®é¢˜
    level = TEST_CONFIG['level']
    problem_id = TEST_CONFIG['problem_id']
    backend = TEST_CONFIG['backend']
    num_perf_trials = TEST_CONFIG['num_perf_trials']

    print(f"\nğŸ“ é…ç½®:")
    print(f"  - Level: {level}")
    print(f"  - Problem ID: {problem_id}")
    print(f"  - Backend: {backend}")
    print(f"  - æ€§èƒ½æµ‹è¯•æ¬¡æ•°: {num_perf_trials}")

    print(f"\nğŸ“¥ ä» HuggingFace åŠ è½½æ•°æ®é›†...")
    dataset = load_dataset(TEST_CONFIG['dataset_name'])
    curr_level_dataset = dataset[f"level_{level}"]

    curr_problem = curr_level_dataset.filter(
        lambda x: x["problem_id"] == problem_id
    )

    if len(curr_problem) == 0:
        print(f"âŒ æ‰¾ä¸åˆ° Level {level} Problem {problem_id}")
        return

    ref_code = curr_problem["code"][0]
    problem_name = curr_problem["name"][0]
    print(f"âœ… é—®é¢˜: {problem_name}")

    # æµ‹é‡å‚è€ƒå®ç°æ€§èƒ½
    print(f"\nâ±ï¸  å¼€å§‹æµ‹é‡å‚è€ƒå®ç°æ€§èƒ½...")
    print(f"   (é¢„çƒ­ 3 æ¬¡ï¼Œæµ‹è¯• {num_perf_trials} æ¬¡)")

    runtime_stats = measure_reference_time(
        ref_code,
        num_trials=num_perf_trials,
        device=device,
        backend=backend,
        verbose=False
    )

    if runtime_stats is None:
        print("âŒ æµ‹é‡å¤±è´¥")
        return

    # ä¿å­˜ç»“æœ
    baseline_data = {
        'level': level,
        'problem_id': problem_id,
        'problem_name': problem_name,
        'backend': backend,
        'device': str(torch.cuda.get_device_name(device)),
        'num_trials': num_perf_trials,
        'runtime_stats': runtime_stats,
        'runtime_mean_us': runtime_stats['mean'],  # å¾®ç§’
        'runtime_mean_ms': runtime_stats['mean'] / 1000.0,  # æ¯«ç§’
    }

    # ä¿å­˜åˆ°æ–‡ä»¶
    output_path = os.path.join(OUTPUT_DIR, 'baseline_time.json')
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(baseline_data, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… Baseline æµ‹é‡å®Œæˆ!")
    print(f"   å¹³å‡æ—¶é—´: {baseline_data['runtime_mean_ms']:.4f} ms")
    print(f"   æ ‡å‡†å·®: {runtime_stats.get('std', 0) / 1000.0:.4f} ms")
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

    print("\nğŸ‰ ç°åœ¨å¯ä»¥è¿è¡Œ python run_kernelbench_wrapper.py æ¥æµ‹è¯•æ¨¡å‹äº†")


if __name__ == "__main__":
    main()
