"""
æµ‹è¯•ä¸åŒ AI æ¨¡å‹å¯¹ Level 1 ç¬¬ä¸€ä¸ªé—®é¢˜çš„ CUDA æ ¸å‡½æ•°ç”Ÿæˆèƒ½åŠ›
"""

import sys
import os
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ  KernelBench åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../KernelBench'))

from datasets import load_dataset
from config import MODELS_TO_TEST, TEST_CONFIG, OUTPUT_DIR

def extract_first_code(text, languages=None):
    """ä» LLM è¾“å‡ºä¸­æå–ä»£ç å—"""
    if languages is None:
        languages = ["python", "cpp", "cuda"]

    # å°è¯•æå–ä»£ç å—
    for lang in languages:
        if f"```{lang}" in text:
            start = text.find(f"```{lang}") + len(f"```{lang}")
            end = text.find("```", start)
            if end != -1:
                return text[start:end].strip()

    # å°è¯•æå–é€šç”¨ä»£ç å—
    if "```" in text:
        start = text.find("```") + 3
        # è·³è¿‡è¯­è¨€æ ‡è¯†ç¬¦
        newline = text.find("\n", start)
        if newline != -1:
            start = newline + 1
        end = text.find("```", start)
        if end != -1:
            return text[start:end].strip()

    return text


def get_problem_from_dataset(level, problem_id):
    """ä» HuggingFace è·å–é—®é¢˜"""
    print(f"\nğŸ“¥ æ­£åœ¨ä» HuggingFace åŠ è½½æ•°æ®é›†...")
    dataset = load_dataset(TEST_CONFIG['dataset_name'])
    curr_level_dataset = dataset[f"level_{level}"]

    # è¿‡æ»¤å‡ºæŒ‡å®šé—®é¢˜
    curr_problem = curr_level_dataset.filter(
        lambda x: x["problem_id"] == problem_id
    )

    if len(curr_problem) == 0:
        raise ValueError(f"æ‰¾ä¸åˆ° Level {level} Problem {problem_id}")

    ref_code = curr_problem["code"][0]
    problem_name = curr_problem["name"][0]

    print(f"âœ… æˆåŠŸåŠ è½½é—®é¢˜: {problem_name}")
    return ref_code, problem_name


def generate_prompt(ref_code, backend='cuda'):
    """ç”Ÿæˆæç¤ºè¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    if backend == 'cuda':
        prompt = f"""You are an expert CUDA programmer. Given the following PyTorch reference implementation, generate an efficient CUDA kernel implementation.

Reference PyTorch code:
```python
{ref_code}
```

Please provide:
1. A complete CUDA implementation with kernel function and wrapper
2. Optimized for performance (use shared memory, coalescing, etc.)
3. The code should be directly compilable and callable from Python using PyTorch

Generate only the CUDA implementation code."""
    else:
        prompt = f"""Given the following PyTorch reference implementation, generate an efficient {backend} implementation.

Reference PyTorch code:
```python
{ref_code}
```

Generate the optimized {backend} implementation code."""

    return prompt


def test_single_model(model_config, ref_code, problem_name):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
    model_name = model_config['name']
    model_id = model_config['model_id']
    client = model_config['client']

    print(f"\nğŸ¤– æµ‹è¯•æ¨¡å‹: {model_name} ({model_id})")
    print(f"ğŸ“ é—®é¢˜: {problem_name}")

    # ç”Ÿæˆæç¤ºè¯
    prompt = generate_prompt(ref_code, TEST_CONFIG['backend'])

    # è°ƒç”¨æ¨¡å‹
    try:
        print(f"â³ æ­£åœ¨ç”Ÿæˆä»£ç ...")
        start_time = datetime.now()

        response = client.chat.completions.create(
            model=model_id,
            messages=[
                {"role": "system", "content": "You are an expert GPU programmer specializing in high-performance CUDA kernels."},
                {"role": "user", "content": prompt}
            ],
            temperature=TEST_CONFIG['temperature'],
            max_tokens=TEST_CONFIG['max_tokens'],
        )

        end_time = datetime.now()
        generation_time = (end_time - start_time).total_seconds()

        # æå–ç”Ÿæˆçš„ä»£ç 
        generated_text = response.choices[0].message.content
        generated_code = extract_first_code(generated_text, ["python", "cpp", "cuda"])

        print(f"âœ… ä»£ç ç”ŸæˆæˆåŠŸï¼è€—æ—¶: {generation_time:.2f} ç§’")
        print(f"ğŸ“Š ç”Ÿæˆä»£ç é•¿åº¦: {len(generated_code)} å­—ç¬¦")

        result = {
            'model_name': model_name,
            'model_id': model_id,
            'success': True,
            'generation_time': generation_time,
            'code_length': len(generated_code),
            'generated_code': generated_code,
            'full_response': generated_text,
            'error': None,
        }

    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
        result = {
            'model_name': model_name,
            'model_id': model_id,
            'success': False,
            'generation_time': None,
            'code_length': 0,
            'generated_code': None,
            'full_response': None,
            'error': str(e),
        }

    return result


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ AI æ¨¡å‹ GPU æ ¸å‡½æ•°ç”Ÿæˆèƒ½åŠ›æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(OUTPUT_DIR)
    output_path.mkdir(exist_ok=True)

    # è·å–é—®é¢˜
    level = TEST_CONFIG['level']
    problem_id = TEST_CONFIG['problem_id']

    try:
        ref_code, problem_name = get_problem_from_dataset(level, problem_id)
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return

    # ä¿å­˜å‚è€ƒä»£ç 
    ref_code_path = output_path / f"reference_level{level}_problem{problem_id}.py"
    with open(ref_code_path, 'w', encoding='utf-8') as f:
        f.write(ref_code)
    print(f"\nğŸ’¾ å‚è€ƒä»£ç å·²ä¿å­˜åˆ°: {ref_code_path}")

    # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
    results = []
    for model_config in MODELS_TO_TEST:
        result = test_single_model(model_config, ref_code, problem_name)
        results.append(result)

        # ä¿å­˜ç”Ÿæˆçš„ä»£ç 
        if result['success']:
            code_filename = f"{model_config['model_id'].replace('/', '_')}_level{level}_problem{problem_id}.py"
            code_path = output_path / code_filename
            with open(code_path, 'w', encoding='utf-8') as f:
                f.write(result['generated_code'])
            print(f"ğŸ’¾ ç”Ÿæˆçš„ä»£ç å·²ä¿å­˜åˆ°: {code_path}")

    # ä¿å­˜å®Œæ•´ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_filename = f"test_results_{timestamp}.json"
    results_path = output_path / results_filename

    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump({
            'test_config': TEST_CONFIG,
            'problem_name': problem_name,
            'results': results,
            'timestamp': timestamp,
        }, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

    # æ‰“å°æ±‡æ€»
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    print(f"é—®é¢˜: Level {level} Problem {problem_id} - {problem_name}")
    print(f"æµ‹è¯•æ¨¡å‹æ•°: {len(MODELS_TO_TEST)}")
    print(f"æˆåŠŸç”Ÿæˆ: {sum(1 for r in results if r['success'])}/{len(results)}")
    print("\nå„æ¨¡å‹è¯¦æƒ…:")
    for result in results:
        status = "âœ…" if result['success'] else "âŒ"
        time_info = f"{result['generation_time']:.2f}s" if result['success'] else "N/A"
        print(f"  {status} {result['model_name']:30s} - {time_info}")

    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
