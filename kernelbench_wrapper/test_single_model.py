"""
æµ‹è¯•å•ä¸ªæ¨¡å‹ - å®Œå…¨ä½¿ç”¨ KernelBench çš„æ–¹æ³•
"""
import sys
import os
from pathlib import Path
from datetime import datetime
import json

# é…ç½® HuggingFace é•œåƒï¼ˆè§£å†³å›½å†…è¿æ¥é—®é¢˜ï¼‰
os.environ['HF_ENDPOINT'] = os.getenv('HF_ENDPOINT', 'https://hf-mirror.com')

# æ·»åŠ  KernelBench åˆ°è·¯å¾„
kernelbench_path = os.path.join(os.path.dirname(__file__), '../../KernelBench')
sys.path.insert(0, kernelbench_path)

from datasets import load_dataset
from src.eval import eval_kernel_against_ref
from src.utils import extract_first_code, set_gpu_arch
from src.prompt_constructor import prompt_generate_custom_cuda_from_prompt_template

from .config import MODELS_TO_TEST, TEST_CONFIG, OUTPUT_DIR
from .custom_inference import create_custom_inference_function


def test_single_model(model_config, ref_code, problem_name, backend='cuda', verbose=False):
    """
    æµ‹è¯•å•ä¸ªæ¨¡å‹

    Args:
        model_config: æ¨¡å‹é…ç½®å­—å…¸
        ref_code: å‚è€ƒä»£ç 
        problem_name: é—®é¢˜åç§°
        backend: åç«¯ç±»å‹ (cuda/triton)
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        dict: æµ‹è¯•ç»“æœ
    """
    model_name = model_config['name']
    model_id = model_config['model_id']

    print(f"\n{'='*70}")
    print(f"ğŸ¤– æµ‹è¯•æ¨¡å‹: {model_name} ({model_id})")
    print(f"ğŸ“ é—®é¢˜: {problem_name}")
    print(f"{'='*70}")

    # 1. ä½¿ç”¨ KernelBench çš„å®˜æ–¹ prompt
    print("â³ ç”Ÿæˆ prompt...")
    if backend == 'cuda':
        prompt = prompt_generate_custom_cuda_from_prompt_template(ref_code)
    else:
        from src.prompt_constructor_multilang import get_prompt_for_backend
        prompt = get_prompt_for_backend(ref_code, backend)

    # 2. ä½¿ç”¨è‡ªå®šä¹‰ API è°ƒç”¨ç”Ÿæˆä»£ç 
    print("â³ è°ƒç”¨æ¨¡å‹ç”Ÿæˆä»£ç ...")
    inference_fn = create_custom_inference_function(
        model_id=model_id,
        temperature=TEST_CONFIG['temperature'],
        max_tokens=TEST_CONFIG['max_tokens'],
        verbose=verbose
    )

    try:
        start_time = datetime.now()
        generated_text = inference_fn(prompt)
        end_time = datetime.now()
        generation_time = (end_time - start_time).total_seconds()

        # 3. ä½¿ç”¨ KernelBench çš„ä»£ç æå–æ–¹æ³•
        generated_code = extract_first_code(generated_text, ["python", "cpp"])

        if not generated_code:
            raise ValueError("æœªèƒ½ä»å“åº”ä¸­æå–ä»£ç ")

        print(f"âœ… ä»£ç ç”ŸæˆæˆåŠŸï¼è€—æ—¶: {generation_time:.2f} ç§’")
        print(f"ğŸ“Š ç”Ÿæˆä»£ç é•¿åº¦: {len(generated_code)} å­—ç¬¦")

    except Exception as e:
        print(f"âŒ ä»£ç ç”Ÿæˆå¤±è´¥: {str(e)}")
        return {
            'model_name': model_name,
            'model_id': model_id,
            'success': False,
            'generation_time': None,
            'generated_code': None,
            'error': str(e),
            'eval_result': None,
        }

    # 4. ä½¿ç”¨ KernelBench çš„å®˜æ–¹è¯„ä¼°æ–¹æ³•
    print(f"\nğŸ” å¼€å§‹è¯„ä¼°ä»£ç ï¼ˆæ­£ç¡®æ€§ + æ€§èƒ½ï¼‰...")
    try:
        eval_result = eval_kernel_against_ref(
            ref_code,
            generated_code,
            verbose=verbose,
            measure_performance=True,
            num_correct_trials=TEST_CONFIG['num_correct_trials'],
            num_perf_trials=TEST_CONFIG['num_perf_trials'],
            backend=backend,
        )

        # KernelExecResult æ˜¯ä¸€ä¸ª Pydantic modelï¼Œéœ€è¦ç›´æ¥è®¿é—®å±æ€§
        is_compiled = eval_result.compiled if eval_result else False
        is_correct = eval_result.correctness if eval_result else False
        custom_time_ms = eval_result.runtime / 1000.0 if (eval_result and eval_result.runtime > 0) else 0.0  # è½¬æ¢ä¸º ms

        # å¦‚æœç¼–è¯‘å¤±è´¥æˆ–æ­£ç¡®æ€§æµ‹è¯•å¤±è´¥
        if not is_compiled or not is_correct:
            print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
            print(f"  âŒ ç¼–è¯‘: {'é€šè¿‡ âœ“' if is_compiled else 'å¤±è´¥ âœ—'}")
            print(f"  âŒ æ­£ç¡®æ€§: {'é€šè¿‡ âœ“' if is_correct else 'å¤±è´¥ âœ—'}")
            if eval_result and eval_result.metadata:
                print(f"  â„¹ï¸  é”™è¯¯ä¿¡æ¯: {eval_result.metadata}")

            return {
                'model_name': model_name,
                'model_id': model_id,
                'success': True,
                'generation_time': generation_time,
                'generated_code': generated_code,
                'full_response': generated_text,
                'error': None,
                'eval_result': {
                    'compiled': is_compiled,
                    'is_correct': is_correct,
                    'speedup': 0.0,
                    'ref_time_ms': 0.0,
                    'custom_time_ms': custom_time_ms,
                    'fast_1': False,
                    'fast_2': False,
                    'metadata': eval_result.metadata if eval_result else {}
                }
            }

        # è¯»å–é¢„å…ˆç”Ÿæˆçš„ baseline æ—¶é—´ï¼ˆç…§æ¬ KernelBench çš„æ–¹å¼ï¼‰
        print("  â±ï¸  è¯»å–å‚è€ƒå®ç° baseline æ—¶é—´...")
        baseline_path = Path(OUTPUT_DIR) / 'baseline_time.json'

        if not baseline_path.exists():
            print(f"  âš ï¸  æ‰¾ä¸åˆ° baseline æ–‡ä»¶: {baseline_path}")
            print(f"  âš ï¸  è¯·å…ˆè¿è¡Œ: python generate_baseline.py")
            print(f"  âš ï¸  å°†ä½¿ç”¨ç”Ÿæˆä»£ç æ—¶é—´ä½œä¸ºåŸºå‡† (åŠ é€Ÿæ¯” = 1.0x)")
            ref_time_ms = custom_time_ms
        else:
            try:
                with open(baseline_path, 'r', encoding='utf-8') as f:
                    baseline_data = json.load(f)
                ref_time_ms = baseline_data['runtime_mean_ms']
                print(f"  âœ… è¯»å–åˆ° baseline: {ref_time_ms:.4f} ms")
            except Exception as e:
                print(f"  âš ï¸  è¯»å– baseline å¤±è´¥: {str(e)}")
                print(f"  âš ï¸  å°†ä½¿ç”¨ç”Ÿæˆä»£ç æ—¶é—´ä½œä¸ºåŸºå‡† (åŠ é€Ÿæ¯” = 1.0x)")
                ref_time_ms = custom_time_ms

        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup = ref_time_ms / custom_time_ms if custom_time_ms > 0 else 0.0

        print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
        print(f"  âœ… ç¼–è¯‘: é€šè¿‡ âœ“")
        print(f"  âœ… æ­£ç¡®æ€§: é€šè¿‡ âœ“")
        print(f"  â±ï¸  PyTorch æ—¶é—´: {ref_time_ms:.4f} ms")
        print(f"  â±ï¸  ç”Ÿæˆä»£ç æ—¶é—´: {custom_time_ms:.4f} ms")
        print(f"  ğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x")
        if speedup > 1.0:
            print(f"  ğŸ‰ æ¯” PyTorch å¿« {(speedup-1)*100:.1f}%!")
        elif speedup < 1.0:
            print(f"  ğŸŒ æ¯” PyTorch æ…¢ {(1-speedup)*100:.1f}%")

        return {
            'model_name': model_name,
            'model_id': model_id,
            'success': True,
            'generation_time': generation_time,
            'generated_code': generated_code,
            'full_response': generated_text,
            'error': None,
            'eval_result': {
                'compiled': is_compiled,
                'is_correct': is_correct,
                'speedup': speedup,
                'ref_time_ms': ref_time_ms,
                'custom_time_ms': custom_time_ms,
                'fast_1': is_correct and speedup > 1.0,
                'fast_2': is_correct and speedup > 2.0,
                'metadata': eval_result.metadata if eval_result else {}
            }
        }

    except Exception as e:
        import traceback
        print(f"âŒ è¯„ä¼°å¤±è´¥: {str(e)}")
        if verbose:
            traceback.print_exc()
        return {
            'model_name': model_name,
            'model_id': model_id,
            'success': True,
            'generation_time': generation_time,
            'generated_code': generated_code,
            'full_response': generated_text,
            'error': f"Evaluation error: {str(e)}",
            'eval_result': None,
        }


def check_msvc_compiler():
    """æ£€æŸ¥ Windows ä¸Šæ˜¯å¦æœ‰ MSVC ç¼–è¯‘å™¨"""
    import subprocess
    import platform

    if platform.system() != 'Windows':
        return True

    try:
        result = subprocess.run(['where', 'cl'], capture_output=True, text=True)
        if result.returncode == 0:
            return True
    except:
        pass

    return False


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ ä½¿ç”¨ KernelBench å®˜æ–¹æ–¹æ³•æµ‹è¯• AI æ¨¡å‹")
    print("   ä»…æ›¿æ¢ API è°ƒç”¨ä¸ºè‡ªå®šä¹‰ä¸­è½¬")
    print("=" * 80)

    # æ£€æŸ¥ GPU
    import torch
    if not torch.cuda.is_available():
        print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° CUDA GPUï¼")
        print("   è¯„ä¼°éœ€è¦ GPU æ‰èƒ½è¿è¡Œã€‚")
        return

    print(f"âœ… æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(0)}")

    # æ£€æŸ¥ç¼–è¯‘å™¨
    if not check_msvc_compiler():
        print("\nâš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° MSVC ç¼–è¯‘å™¨ (cl.exe)ï¼")
        print("   Windows ä¸Šç¼–è¯‘ CUDA æ‰©å±•éœ€è¦ Visual Studioã€‚")
        print("   è¯·å®‰è£… Visual Studio 2019/2022 (åŒ…å« C++ å¼€å‘å·¥å…·)ã€‚")
        print("\n   æˆ–è€…ä½¿ç”¨ 'Developer Command Prompt for VS' è¿è¡Œæ­¤è„šæœ¬ã€‚")
        response = input("\næ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
        if response.lower() != 'y':
            print("âŒ æµ‹è¯•å–æ¶ˆ")
            return

    # è®¾ç½® GPU æ¶æ„
    if TEST_CONFIG.get('gpu_arch'):
        set_gpu_arch(TEST_CONFIG['gpu_arch'])

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(OUTPUT_DIR)
    output_path.mkdir(parents=True, exist_ok=True)

    # è·å–é—®é¢˜
    level = TEST_CONFIG['level']
    problem_id = TEST_CONFIG['problem_id']
    backend = TEST_CONFIG['backend']

    print(f"\nğŸ“¥ æ­£åœ¨ä» HuggingFace åŠ è½½æ•°æ®é›†...")
    dataset = load_dataset(TEST_CONFIG['dataset_name'])
    curr_level_dataset = dataset[f"level_{level}"]

    curr_problem = curr_level_dataset.filter(
        lambda x: x["problem_id"] == problem_id
    )

    if len(curr_problem) == 0:
        print(f"âŒ æ‰¾ä¸åˆ° Level {level} Problem {problem_id}")
        return

    ref_code = curr_problem["code"][0]
    problem_name = curr_problem["name"][0]
    print(f"âœ… æˆåŠŸåŠ è½½é—®é¢˜: {problem_name}")

    # ä¿å­˜å‚è€ƒä»£ç 
    ref_code_path = output_path / f"reference_level{level}_problem{problem_id}.py"
    with open(ref_code_path, 'w', encoding='utf-8') as f:
        f.write(ref_code)
    print(f"ğŸ’¾ å‚è€ƒä»£ç å·²ä¿å­˜åˆ°: {ref_code_path}")

    # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
    results = []
    for i, model_config in enumerate(MODELS_TO_TEST, 1):
        print(f"\n{'#'*80}")
        print(f"# æµ‹è¯•è¿›åº¦: {i}/{len(MODELS_TO_TEST)}")
        print(f"{'#'*80}")

        result = test_single_model(
            model_config,
            ref_code,
            problem_name,
            backend=backend,
            verbose=False
        )
        results.append(result)

        # ä¿å­˜ç”Ÿæˆçš„ä»£ç 
        if result['success'] and result['generated_code']:
            code_filename = f"{model_config['model_id'].replace('/', '_')}_level{level}_problem{problem_id}.py"
            code_path = output_path / code_filename
            with open(code_path, 'w', encoding='utf-8') as f:
                f.write(result['generated_code'])
            print(f"ğŸ’¾ ç”Ÿæˆçš„ä»£ç å·²ä¿å­˜åˆ°: {code_path}")

    # ä¿å­˜å®Œæ•´ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_filename = f"test_results_{timestamp}.json"
    results_path = output_path / results_filename

    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump({
            'test_config': TEST_CONFIG,
            'problem_name': problem_name,
            'results': results,
            'timestamp': timestamp,
        }, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

    # æ‰“å°æ±‡æ€»
    print("\n" + "=" * 80)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 80)
    print(f"é—®é¢˜: Level {level} Problem {problem_id} - {problem_name}")
    print(f"Backend: {backend}")

    success_count = sum(1 for r in results if r['success'])
    correct_count = sum(1 for r in results if r.get('eval_result') and r['eval_result'].get('is_correct'))
    fast_1_count = sum(1 for r in results if r.get('eval_result') and r['eval_result'].get('fast_1'))
    fast_2_count = sum(1 for r in results if r.get('eval_result') and r['eval_result'].get('fast_2'))

    print(f"\næ€»ä½“ç»Ÿè®¡:")
    print(f"  ğŸ“ æµ‹è¯•æ¨¡å‹æ•°: {len(results)}")
    print(f"  âœ… æˆåŠŸç”Ÿæˆ: {success_count}/{len(results)} ({success_count/len(results)*100:.1f}%)")
    print(f"  âœ“  æ­£ç¡®æ€§é€šè¿‡: {correct_count}/{len(results)} ({correct_count/len(results)*100:.1f}%)")
    print(f"  ğŸš€ fast_1 (å¿«äºPyTorch): {fast_1_count}/{len(results)} ({fast_1_count/len(results)*100:.1f}%)")
    print(f"  ğŸ† fast_2 (å¿«2å€ä»¥ä¸Š): {fast_2_count}/{len(results)} ({fast_2_count/len(results)*100:.1f}%)")

    print(f"\nå„æ¨¡å‹è¯¦æƒ…:")
    print(f"{'æ¨¡å‹':<35} {'ç”Ÿæˆ':<8} {'æ­£ç¡®':<8} {'åŠ é€Ÿæ¯”':<10} {'fast_1':<8}")
    print("-" * 80)
    for result in results:
        name = result['model_name'][:33]
        gen_status = "âœ…" if result['success'] else "âŒ"

        if result.get('eval_result'):
            eval_res = result['eval_result']
            correct = "âœ…" if eval_res.get('is_correct') else "âŒ"
            speedup = f"{eval_res.get('speedup', 0):.2f}x"
            fast1 = "âœ…" if eval_res.get('fast_1') else "âŒ"
        else:
            correct = "N/A"
            speedup = "N/A"
            fast1 = "N/A"

        print(f"{name:<35} {gen_status:<8} {correct:<8} {speedup:<10} {fast1:<8}")

    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
