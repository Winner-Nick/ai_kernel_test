"""
æµ‹è¯•ä¸åŒ AI æ¨¡å‹å¯¹ Level 1 ç¬¬ä¸€ä¸ªé—®é¢˜çš„ CUDA æ ¸å‡½æ•°ç”Ÿæˆèƒ½åŠ›
ä½¿ç”¨ KernelBench çš„å®Œæ•´è¯„ä¼°æ–¹æ³•ï¼ˆæ­£ç¡®æ€§ + æ€§èƒ½ï¼‰
"""

import sys
import os
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ  KernelBench åˆ°è·¯å¾„
kernelbench_path = os.path.join(os.path.dirname(__file__), '../KernelBench')
sys.path.insert(0, kernelbench_path)

from datasets import load_dataset
from config import MODELS_TO_TEST, TEST_CONFIG, OUTPUT_DIR

# ä» KernelBench å¯¼å…¥è¯„ä¼°å‡½æ•°
from src.eval import eval_kernel_against_ref
from src.utils import extract_first_code, set_gpu_arch


def get_problem_from_dataset(level, problem_id):
    """ä» HuggingFace è·å–é—®é¢˜"""
    print(f"\nğŸ“¥ æ­£åœ¨ä» HuggingFace åŠ è½½æ•°æ®é›†...")
    dataset = load_dataset(TEST_CONFIG['dataset_name'])
    curr_level_dataset = dataset[f"level_{level}"]

    # è¿‡æ»¤å‡ºæŒ‡å®šé—®é¢˜
    curr_problem = curr_level_dataset.filter(
        lambda x: x["problem_id"] == problem_id
    )

    if len(curr_problem) == 0:
        raise ValueError(f"æ‰¾ä¸åˆ° Level {level} Problem {problem_id}")

    ref_code = curr_problem["code"][0]
    problem_name = curr_problem["name"][0]

    print(f"âœ… æˆåŠŸåŠ è½½é—®é¢˜: {problem_name}")
    return ref_code, problem_name


def generate_prompt(ref_code, backend='cuda'):
    """ç”Ÿæˆæç¤ºè¯"""
    if backend == 'cuda':
        prompt = f"""You are an expert CUDA programmer. Given the following PyTorch reference implementation, generate an optimized version using custom CUDA kernels.

Reference PyTorch code:
```python
{ref_code}
```

Requirements:
1. Output must be VALID PYTHON CODE that can be directly imported and executed
2. Use torch.utils.cpp_extension.load_inline() to compile CUDA kernels inline
3. Put CUDA kernel code in a Python string variable (e.g., cuda_source = \"\"\"...\"\"\")
4. Create a Model class (or ModelNew) that uses the compiled kernel
5. Include get_inputs() function from the reference code
6. Optimize for performance using shared memory, tiling, and memory coalescing

Example structure:
```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

cuda_source = \"\"\"
#include <torch/extension.h>
__global__ void my_kernel(...) {{
    // Your CUDA kernel code
}}
torch::Tensor my_function(...) {{
    // C++ wrapper that launches kernel
}}
\"\"\"

cpp_source = \"\"\"
torch::Tensor my_function(...);
\"\"\"

custom_module = load_inline(
    name='custom_op',
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=['my_function'],
    verbose=True
)

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.custom_module = custom_module

    def forward(self, ...):
        return self.custom_module.my_function(...)

def get_inputs():
    # Same as reference implementation
    return [...]
```

Generate the complete Python code with inline CUDA kernel."""

    elif backend == 'triton':
        prompt = f"""You are an expert Triton programmer. Given the following PyTorch reference implementation, generate an efficient Triton kernel implementation.

Reference PyTorch code:
```python
{ref_code}
```

Requirements:
1. Generate a complete Triton implementation
2. Optimize for performance using Triton's features
3. Match the exact interface and behavior of the reference implementation

Generate the complete Triton implementation code."""

    else:
        prompt = f"""Given the following PyTorch reference implementation, generate an efficient {backend} implementation.

Reference PyTorch code:
```python
{ref_code}
```

Generate the optimized {backend} implementation code that matches the reference behavior."""

    return prompt


def test_single_model(model_config, ref_code, problem_name, backend='cuda', verbose=False):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹ï¼ˆåŒ…å«å®Œæ•´è¯„ä¼°ï¼‰"""
    model_name = model_config['name']
    model_id = model_config['model_id']
    client = model_config['client']

    print(f"\n{'='*70}")
    print(f"ğŸ¤– æµ‹è¯•æ¨¡å‹: {model_name} ({model_id})")
    print(f"ğŸ“ é—®é¢˜: {problem_name}")
    print(f"{'='*70}")

    # ç”Ÿæˆæç¤ºè¯
    prompt = generate_prompt(ref_code, backend)

    # è°ƒç”¨æ¨¡å‹ç”Ÿæˆä»£ç 
    try:
        print(f"â³ æ­£åœ¨ç”Ÿæˆä»£ç ...")
        start_time = datetime.now()

        response = client.chat.completions.create(
            model=model_id,
            messages=[
                {"role": "system", "content": "You are an expert GPU programmer specializing in high-performance kernel optimization."},
                {"role": "user", "content": prompt}
            ],
            temperature=TEST_CONFIG['temperature'],
            max_tokens=TEST_CONFIG['max_tokens'],
        )

        end_time = datetime.now()
        generation_time = (end_time - start_time).total_seconds()

        # æå–ç”Ÿæˆçš„ä»£ç 
        generated_text = response.choices[0].message.content
        generated_code = extract_first_code(generated_text, ["python", "cpp", "cuda"])

        if not generated_code:
            raise ValueError("æœªèƒ½ä»å“åº”ä¸­æå–ä»£ç ")

        print(f"âœ… ä»£ç ç”ŸæˆæˆåŠŸï¼è€—æ—¶: {generation_time:.2f} ç§’")
        print(f"ğŸ“Š ç”Ÿæˆä»£ç é•¿åº¦: {len(generated_code)} å­—ç¬¦")

    except Exception as e:
        print(f"âŒ ä»£ç ç”Ÿæˆå¤±è´¥: {str(e)}")
        return {
            'model_name': model_name,
            'model_id': model_id,
            'success': False,
            'generation_time': None,
            'generated_code': None,
            'error': str(e),
            'eval_result': None,
        }

    # ä½¿ç”¨ KernelBench çš„è¯„ä¼°å‡½æ•°è¿›è¡Œå®Œæ•´è¯„ä¼°
    print(f"\nğŸ” å¼€å§‹è¯„ä¼°ä»£ç ï¼ˆæ­£ç¡®æ€§ + æ€§èƒ½ï¼‰...")
    try:
        eval_result = eval_kernel_against_ref(
            ref_code,
            generated_code,
            verbose=verbose,
            measure_performance=True,
            num_correct_trials=5,  # æ­£ç¡®æ€§æµ‹è¯•æ¬¡æ•°
            num_perf_trials=100,   # æ€§èƒ½æµ‹è¯•æ¬¡æ•°
            backend=backend,
        )

        # è§£æè¯„ä¼°ç»“æœ
        is_correct = eval_result.get('is_correct', False)
        speedup = eval_result.get('speedup', 0.0)
        ref_time = eval_result.get('ref_time_ms', 0.0)
        custom_time = eval_result.get('custom_time_ms', 0.0)

        print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
        print(f"  âœ… æ­£ç¡®æ€§: {'é€šè¿‡ âœ“' if is_correct else 'å¤±è´¥ âœ—'}")
        if is_correct:
            print(f"  â±ï¸  PyTorch æ—¶é—´: {ref_time:.4f} ms")
            print(f"  â±ï¸  ç”Ÿæˆä»£ç æ—¶é—´: {custom_time:.4f} ms")
            print(f"  ğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x")
            if speedup > 1.0:
                print(f"  ğŸ‰ æ¯” PyTorch å¿« {(speedup-1)*100:.1f}%!")
            elif speedup < 1.0:
                print(f"  ğŸŒ æ¯” PyTorch æ…¢ {(1-speedup)*100:.1f}%")
            else:
                print(f"  âš–ï¸  ä¸ PyTorch é€Ÿåº¦ç›¸å½“")

        return {
            'model_name': model_name,
            'model_id': model_id,
            'success': True,
            'generation_time': generation_time,
            'generated_code': generated_code,
            'full_response': generated_text,
            'error': None,
            'eval_result': {
                'is_correct': is_correct,
                'speedup': speedup,
                'ref_time_ms': ref_time,
                'custom_time_ms': custom_time,
                'fast_1': is_correct and speedup > 1.0,  # æ­£ç¡®ä¸”å¿«äºPyTorch
                'fast_2': is_correct and speedup > 2.0,  # æ­£ç¡®ä¸”å¿«2å€ä»¥ä¸Š
            }
        }

    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {str(e)}")
        return {
            'model_name': model_name,
            'model_id': model_id,
            'success': True,  # ç”ŸæˆæˆåŠŸï¼Œä½†è¯„ä¼°å¤±è´¥
            'generation_time': generation_time,
            'generated_code': generated_code,
            'full_response': generated_text,
            'error': f"Evaluation error: {str(e)}",
            'eval_result': None,
        }


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ AI æ¨¡å‹ GPU æ ¸å‡½æ•°ç”Ÿæˆèƒ½åŠ›å®Œæ•´æµ‹è¯•")
    print("   åŒ…å« KernelBench å®˜æ–¹è¯„ä¼°ï¼ˆæ­£ç¡®æ€§ + æ€§èƒ½ï¼‰")
    print("=" * 80)

    # æ£€æŸ¥ GPU
    import torch
    if not torch.cuda.is_available():
        print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° CUDA GPUï¼")
        print("   è¯„ä¼°éœ€è¦ GPU æ‰èƒ½è¿è¡Œã€‚")
        response = input("æ˜¯å¦ç»§ç»­ï¼ˆåªç”Ÿæˆä»£ç ï¼Œè·³è¿‡è¯„ä¼°ï¼‰ï¼Ÿ(y/n): ")
        if response.lower() != 'y':
            print("âŒ æµ‹è¯•å–æ¶ˆ")
            return
        skip_eval = True
    else:
        skip_eval = False
        print(f"âœ… æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(0)}")
        # è®¾ç½® GPU æ¶æ„
        if TEST_CONFIG.get('gpu_arch'):
            set_gpu_arch(TEST_CONFIG['gpu_arch'])

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(OUTPUT_DIR)
    output_path.mkdir(exist_ok=True)

    # è·å–é—®é¢˜
    level = TEST_CONFIG['level']
    problem_id = TEST_CONFIG['problem_id']
    backend = TEST_CONFIG['backend']

    try:
        ref_code, problem_name = get_problem_from_dataset(level, problem_id)
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return

    # ä¿å­˜å‚è€ƒä»£ç 
    ref_code_path = output_path / f"reference_level{level}_problem{problem_id}.py"
    with open(ref_code_path, 'w', encoding='utf-8') as f:
        f.write(ref_code)
    print(f"\nğŸ’¾ å‚è€ƒä»£ç å·²ä¿å­˜åˆ°: {ref_code_path}")

    # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
    results = []
    for i, model_config in enumerate(MODELS_TO_TEST, 1):
        print(f"\n{'#'*80}")
        print(f"# æµ‹è¯•è¿›åº¦: {i}/{len(MODELS_TO_TEST)}")
        print(f"{'#'*80}")

        if skip_eval:
            print("âš ï¸  è·³è¿‡è¯„ä¼°ï¼ˆæ—  GPUï¼‰")
            # TODO: å®ç°åªç”Ÿæˆä¸è¯„ä¼°çš„ç‰ˆæœ¬
            continue

        result = test_single_model(
            model_config,
            ref_code,
            problem_name,
            backend=backend,
            verbose=False
        )
        results.append(result)

        # ä¿å­˜ç”Ÿæˆçš„ä»£ç 
        if result['success'] and result['generated_code']:
            code_filename = f"{model_config['model_id'].replace('/', '_')}_level{level}_problem{problem_id}.py"
            code_path = output_path / code_filename
            with open(code_path, 'w', encoding='utf-8') as f:
                f.write(result['generated_code'])
            print(f"ğŸ’¾ ç”Ÿæˆçš„ä»£ç å·²ä¿å­˜åˆ°: {code_path}")

    # ä¿å­˜å®Œæ•´ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_filename = f"test_results_with_eval_{timestamp}.json"
    results_path = output_path / results_filename

    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump({
            'test_config': TEST_CONFIG,
            'problem_name': problem_name,
            'results': results,
            'timestamp': timestamp,
        }, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

    # æ‰“å°æ±‡æ€»
    print("\n" + "=" * 80)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 80)
    print(f"é—®é¢˜: Level {level} Problem {problem_id} - {problem_name}")
    print(f"Backend: {backend}")
    print(f"\næ¨¡å‹æµ‹è¯•ç»“æœ:")

    success_count = sum(1 for r in results if r['success'])
    correct_count = sum(1 for r in results if r.get('eval_result') and r['eval_result'].get('is_correct'))
    fast_1_count = sum(1 for r in results if r.get('eval_result') and r['eval_result'].get('fast_1'))
    fast_2_count = sum(1 for r in results if r.get('eval_result') and r['eval_result'].get('fast_2'))

    print(f"\næ€»ä½“ç»Ÿè®¡:")
    print(f"  ğŸ“ æµ‹è¯•æ¨¡å‹æ•°: {len(results)}")
    print(f"  âœ… æˆåŠŸç”Ÿæˆ: {success_count}/{len(results)} ({success_count/len(results)*100:.1f}%)")
    print(f"  âœ“  æ­£ç¡®æ€§é€šè¿‡: {correct_count}/{len(results)} ({correct_count/len(results)*100:.1f}%)")
    print(f"  ğŸš€ fast_1 (å¿«äºPyTorch): {fast_1_count}/{len(results)} ({fast_1_count/len(results)*100:.1f}%)")
    print(f"  ğŸ† fast_2 (å¿«2å€ä»¥ä¸Š): {fast_2_count}/{len(results)} ({fast_2_count/len(results)*100:.1f}%)")

    print(f"\nå„æ¨¡å‹è¯¦æƒ…:")
    print(f"{'æ¨¡å‹':<35} {'ç”Ÿæˆ':<8} {'æ­£ç¡®':<8} {'åŠ é€Ÿæ¯”':<10} {'fast_1':<8}")
    print("-" * 80)
    for result in results:
        name = result['model_name'][:33]
        gen_status = "âœ…" if result['success'] else "âŒ"

        if result.get('eval_result'):
            eval_res = result['eval_result']
            correct = "âœ…" if eval_res.get('is_correct') else "âŒ"
            speedup = f"{eval_res.get('speedup', 0):.2f}x"
            fast1 = "âœ…" if eval_res.get('fast_1') else "âŒ"
        else:
            correct = "N/A"
            speedup = "N/A"
            fast1 = "N/A"

        print(f"{name:<35} {gen_status:<8} {correct:<8} {speedup:<10} {fast1:<8}")

    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ’¡ æç¤º: è¿è¡Œ 'python analyze_results_with_eval.py' æŸ¥çœ‹è¯¦ç»†åˆ†æ")


if __name__ == "__main__":
    main()
