"""
åˆ†æå’Œå¯¹æ¯”ä¸åŒæ¨¡å‹çš„ç”Ÿæˆç»“æœ
"""

import json
import os
from pathlib import Path
from datetime import datetime
import re


def load_latest_results(results_dir='./results'):
    """åŠ è½½æœ€æ–°çš„æµ‹è¯•ç»“æœ"""
    results_path = Path(results_dir)
    if not results_path.exists():
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
        return None

    # æ‰¾åˆ°æœ€æ–°çš„ç»“æœæ–‡ä»¶
    result_files = list(results_path.glob('test_results_*.json'))
    if not result_files:
        print(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•ç»“æœæ–‡ä»¶")
        return None

    latest_file = max(result_files, key=lambda p: p.stat().st_mtime)
    print(f"ğŸ“‚ åŠ è½½ç»“æœæ–‡ä»¶: {latest_file.name}")

    with open(latest_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def analyze_code_quality(code):
    """ç®€å•åˆ†æä»£ç è´¨é‡æŒ‡æ ‡"""
    if not code:
        return {}

    metrics = {
        'lines': len(code.split('\n')),
        'chars': len(code),
        'has_kernel': '__global__' in code or '@triton.jit' in code,
        'has_shared_mem': '__shared__' in code,
        'has_sync': '__syncthreads()' in code,
        'has_optimization_comment': any(keyword in code.lower() for keyword in
                                       ['optimize', 'coalesce', 'shared memory', 'tiling']),
    }

    return metrics


def compare_models(results_data):
    """å¯¹æ¯”ä¸åŒæ¨¡å‹çš„è¡¨ç°"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¨¡å‹å¯¹æ¯”åˆ†ææŠ¥å‘Š")
    print("=" * 80)

    problem_name = results_data.get('problem_name', 'Unknown')
    test_config = results_data.get('test_config', {})
    results = results_data.get('results', [])

    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {problem_name}")
    print(f"ğŸ¯ Level: {test_config.get('level')}, Problem ID: {test_config.get('problem_id')}")
    print(f"ğŸ”§ Backend: {test_config.get('backend')}")
    print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {results_data.get('timestamp')}")

    # ç»Ÿè®¡æˆåŠŸç‡
    total_models = len(results)
    successful = sum(1 for r in results if r['success'])
    success_rate = (successful / total_models * 100) if total_models > 0 else 0

    print(f"\nâœ… æˆåŠŸç‡: {successful}/{total_models} ({success_rate:.1f}%)")

    # è¯¦ç»†å¯¹æ¯”è¡¨
    print("\n" + "-" * 80)
    print(f"{'æ¨¡å‹åç§°':<30} {'çŠ¶æ€':<8} {'è€—æ—¶(s)':<10} {'ä»£ç è¡Œæ•°':<10} {'æœ‰æ ¸å‡½æ•°':<10}")
    print("-" * 80)

    for result in results:
        model_name = result['model_name'][:28]
        status = "âœ… æˆåŠŸ" if result['success'] else "âŒ å¤±è´¥"
        time_str = f"{result['generation_time']:.2f}" if result['success'] else "N/A"

        if result['success']:
            metrics = analyze_code_quality(result['generated_code'])
            lines = metrics['lines']
            has_kernel = "æ˜¯" if metrics['has_kernel'] else "å¦"
        else:
            lines = "N/A"
            has_kernel = "N/A"

        print(f"{model_name:<30} {status:<8} {time_str:<10} {lines:<10} {has_kernel:<10}")

    # ä»£ç è´¨é‡è¯¦ç»†åˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ” ä»£ç è´¨é‡è¯¦ç»†åˆ†æ")
    print("=" * 80)

    for result in results:
        if not result['success']:
            continue

        print(f"\n{'='*60}")
        print(f"æ¨¡å‹: {result['model_name']}")
        print(f"{'='*60}")

        metrics = analyze_code_quality(result['generated_code'])

        print(f"  ğŸ“ ä»£ç è¡Œæ•°: {metrics['lines']}")
        print(f"  ğŸ“ å­—ç¬¦æ•°: {metrics['chars']}")
        print(f"  ğŸ¯ åŒ…å«æ ¸å‡½æ•°æ ‡è®°: {'âœ…' if metrics['has_kernel'] else 'âŒ'}")
        print(f"  ğŸ’¾ ä½¿ç”¨å…±äº«å†…å­˜: {'âœ…' if metrics['has_shared_mem'] else 'âŒ'}")
        print(f"  ğŸ”„ åŒ…å«åŒæ­¥æ“ä½œ: {'âœ…' if metrics['has_sync'] else 'âŒ'}")
        print(f"  ğŸ’¡ åŒ…å«ä¼˜åŒ–æ³¨é‡Š: {'âœ…' if metrics['has_optimization_comment'] else 'âŒ'}")

        # æ˜¾ç¤ºä»£ç ç‰‡æ®µï¼ˆå‰10è¡Œï¼‰
        all_code_lines = result['generated_code'].split('\n')
        code_lines = all_code_lines[:10]
        print(f"\n  ğŸ“„ ä»£ç é¢„è§ˆï¼ˆå‰10è¡Œï¼‰:")
        for i, line in enumerate(code_lines, 1):
            print(f"     {i:2d} | {line}")
        if len(all_code_lines) > 10:
            remaining_lines = len(all_code_lines) - 10
            print(f"     ... (è¿˜æœ‰ {remaining_lines} è¡Œ)")

    # ç”Ÿæˆæ—¶é—´å¯¹æ¯”
    print("\n" + "=" * 80)
    print("â±ï¸  ç”Ÿæˆé€Ÿåº¦å¯¹æ¯”")
    print("=" * 80)

    successful_results = [r for r in results if r['success']]
    if successful_results:
        successful_results.sort(key=lambda x: x['generation_time'])

        fastest = successful_results[0]
        slowest = successful_results[-1]
        avg_time = sum(r['generation_time'] for r in successful_results) / len(successful_results)

        print(f"\nğŸ† æœ€å¿«: {fastest['model_name']} - {fastest['generation_time']:.2f}s")
        print(f"ğŸŒ æœ€æ…¢: {slowest['model_name']} - {slowest['generation_time']:.2f}s")
        print(f"ğŸ“Š å¹³å‡: {avg_time:.2f}s")

        # ç»˜åˆ¶ç®€å•çš„æ¡å½¢å›¾
        print("\né€Ÿåº¦æ¡å½¢å›¾:")
        max_time = max(r['generation_time'] for r in successful_results)
        for result in successful_results:
            bar_length = int((result['generation_time'] / max_time) * 40)
            bar = 'â–ˆ' * bar_length
            print(f"  {result['model_name'][:25]:<25} {bar} {result['generation_time']:.2f}s")

    # é”™è¯¯åˆ†æ
    failed_results = [r for r in results if not r['success']]
    if failed_results:
        print("\n" + "=" * 80)
        print("âŒ å¤±è´¥åˆ†æ")
        print("=" * 80)
        for result in failed_results:
            print(f"\næ¨¡å‹: {result['model_name']}")
            print(f"é”™è¯¯: {result['error']}")

    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 80)


def generate_markdown_report(results_data, output_file='results/report.md'):
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# AI æ¨¡å‹ GPU æ ¸å‡½æ•°ç”Ÿæˆèƒ½åŠ›æµ‹è¯•æŠ¥å‘Š\n\n")

        problem_name = results_data.get('problem_name', 'Unknown')
        test_config = results_data.get('test_config', {})
        results = results_data.get('results', [])

        f.write(f"## æµ‹è¯•ä¿¡æ¯\n\n")
        f.write(f"- **é—®é¢˜**: {problem_name}\n")
        f.write(f"- **Level**: {test_config.get('level')}\n")
        f.write(f"- **Problem ID**: {test_config.get('problem_id')}\n")
        f.write(f"- **Backend**: {test_config.get('backend')}\n")
        f.write(f"- **æµ‹è¯•æ—¶é—´**: {results_data.get('timestamp')}\n\n")

        f.write(f"## ç»“æœæ±‡æ€»\n\n")
        successful = sum(1 for r in results if r['success'])
        total = len(results)
        f.write(f"- æµ‹è¯•æ¨¡å‹æ•°: {total}\n")
        f.write(f"- æˆåŠŸç”Ÿæˆ: {successful}/{total} ({successful/total*100:.1f}%)\n\n")

        f.write(f"## è¯¦ç»†ç»“æœ\n\n")
        f.write("| æ¨¡å‹åç§° | çŠ¶æ€ | ç”Ÿæˆæ—¶é—´(s) | ä»£ç è¡Œæ•° | åŒ…å«æ ¸å‡½æ•° |\n")
        f.write("|---------|------|-----------|---------|----------|\n")

        for result in results:
            status = "âœ…" if result['success'] else "âŒ"
            time_str = f"{result['generation_time']:.2f}" if result['success'] else "N/A"

            if result['success']:
                metrics = analyze_code_quality(result['generated_code'])
                lines = metrics['lines']
                has_kernel = "æ˜¯" if metrics['has_kernel'] else "å¦"
            else:
                lines = "N/A"
                has_kernel = "N/A"

            f.write(f"| {result['model_name']} | {status} | {time_str} | {lines} | {has_kernel} |\n")

        f.write(f"\n---\n\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")

    print(f"\nğŸ“ Markdown æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    results_data = load_latest_results()

    if not results_data:
        print("âŒ æ— æ³•åŠ è½½æµ‹è¯•ç»“æœ")
        return

    # ç”Ÿæˆæ§åˆ¶å°åˆ†ææŠ¥å‘Š
    compare_models(results_data)

    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    generate_markdown_report(results_data)


if __name__ == "__main__":
    main()
